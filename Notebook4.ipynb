{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "env = MiniMazeEnv(size=5)\n",
    "\n",
    "# Hyperparameters\n",
    "episodes = 10\n",
    "epsilon = 0.9\n",
    "lr = 0.01\n",
    "gamma = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qlearning - Off-Policy\n",
    "class QLearning():\n",
    "    def __init__(self, env, epsilon, lr, gamma):\n",
    "        self.env = env\n",
    "        # self.Q_table = np.zeros((env.state_space_n, env.action_space_n))\n",
    "        self.Q_table = np.random.rand(env.state_space_n, env.action_space_n)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        \n",
    "    def policy(self, state):\n",
    "        # epsilon greedy\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space_sample()\n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state, :])\n",
    "        \n",
    "    def update(self, old_state, old_action, new_state, reward):\n",
    "        prediction = self.Q_table[old_state, old_action]\n",
    "        target = reward + self.gamma * np.max(self.Q_table[new_state, :])\n",
    "        self.Q_table[old_state, old_action] = prediction + self.lr * (target - prediction)\n",
    "\n",
    "agent_qlearning = QLearning(env, epsilon, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Before Training\n",
      "→ → → → G\n",
      "→ → → → ↑\n",
      "→ → → ↑ ↑\n",
      "→ → → ↑ ↑\n",
      "S ↑ ↑ ↑ ↑\n",
      "Policy After Training\n",
      "→ → → → G\n",
      "→ → → → ↑\n",
      "→ → → ↑ ↑\n",
      "→ → → ↑ ↑\n",
      "S ↑ ↑ ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "print('Policy Before Training')\n",
    "maze_policy = visualize_policy(agent_qlearning.Q_table, (5, 5))\n",
    "for row in maze_policy:\n",
    "    print(' '.join(row))\n",
    "total_rewards_per_episode = []\n",
    "\n",
    "# Training Loop\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent_qlearning.policy(state)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        agent_qlearning.update(state, action, new_state, reward)\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "\n",
    "\n",
    "# Policy After Training\n",
    "print('Policy After Training')\n",
    "maze_policy = visualize_policy(agent_qlearning.Q_table, (5, 5))\n",
    "for row in maze_policy:\n",
    "    print(' '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym-like Maze Environment\n",
    "class MiniMazeEnv:\n",
    "    \"\"\"\n",
    "    Mini maze environment with dynamic size and 4 discrete actions.\n",
    "    Actions are: 0: Left, 1: Down, 2: Right, 3: Up\n",
    "    Starts at the bottom left corner and the top right corner \n",
    "    gives a reward of 1 and is a terminal state.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.state_space_n = size * size\n",
    "        self.action_space_n = 4\n",
    "        \n",
    "        # Start at bottom left\n",
    "        self.start_state = 0\n",
    "        self.goal_state = self.state_space_n - 1\n",
    "        self.state = self.start_state\n",
    "        \n",
    "        self.rewards = np.zeros(self.state_space_n)\n",
    "        # Top right corner reward\n",
    "        self.rewards[self.goal_state] = 1\n",
    "        \n",
    "        # Dictionary where the keys are each state in maze and the values are \n",
    "        # another dictionary where those keys are the actions and the values are the \n",
    "        # next states. \n",
    "        self.transitions = {\n",
    "            s: {\n",
    "                a: self._get_next_state(s, a) for a in range(self.action_space_n)\n",
    "                } for s in range(self.state_space_n)\n",
    "            }\n",
    "        self.terminal_states = [self.goal_state]\n",
    "\n",
    "    def _get_next_state(self, state, action):\n",
    "        row, col = divmod(state, self.size)\n",
    "        # Counter clockwise orientation\n",
    "        if action == 0:  # Left\n",
    "            col = max(col - 1, 0)\n",
    "        elif action == 1:  # Down\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 2:  # Right\n",
    "            col = min(col + 1, self.size - 1)\n",
    "        elif action == 3:  # Up\n",
    "            row = min(row + 1, self.size - 1)\n",
    "\n",
    "        return row * self.size + col\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.state in self.terminal_states:\n",
    "            return self.state, self.rewards[self.state], True, {}\n",
    "        next_state = self.transitions[self.state][action]\n",
    "        reward = self.rewards[next_state]\n",
    "        self.state = next_state\n",
    "        done = self.state in self.terminal_states\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((self.size, self.size), '.')\n",
    "        row, col = divmod(self.state, self.size)\n",
    "        # Agent position\n",
    "        grid[self.size - 1 - row, col] = 'A'\n",
    "        # Reward position\n",
    "        grid[0, self.size - 1] = 'R' if self.state != self.goal_state else 'A'\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        print('\\n')\n",
    "        \n",
    "    def action_space_sample(self):\n",
    "        return np.random.choice(self.action_space_n)\n",
    "\n",
    "\n",
    "# env = MiniMazeEnv(size=3)\n",
    "# state = env.reset()\n",
    "# env.render()\n",
    "# env.step(2)\n",
    "# env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↑ ↑ G\n",
      "↓ ↑ ←\n",
      "S ↑ ←\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def visualize_policy(Q_table, shape):\n",
    "    action_symbols = ['←', '↓', '→', '↑']\n",
    "\n",
    "    policy = np.argmax(Q_table, axis=1)\n",
    "\n",
    "    grid = [['' for _ in range(shape[1])] for _ in range(shape[0])]\n",
    "\n",
    "    for state in range(shape[0] * shape[1]):\n",
    "        row, col = divmod(state, shape[1])\n",
    "        row = shape[0] - 1 - row\n",
    "        if state == 0:\n",
    "            grid[row][col] = 'S'\n",
    "        elif state == shape[0] * shape[1] - 1:\n",
    "            grid[row][col] = 'G'\n",
    "        else:\n",
    "            grid[row][col] = action_symbols[policy[state]]\n",
    "\n",
    "    return grid\n",
    "\n",
    "np.random.seed(0)\n",
    "dummy_Q_table = np.random.random((9, 4))\n",
    "\n",
    "# Visualize the policy\n",
    "maze_policy = visualize_policy(dummy_Q_table, (3, 3))\n",
    "for row in maze_policy:\n",
    "    print(' '.join(row))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
